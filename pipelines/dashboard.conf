input {
    jdbc {
        # ${JDBC_DRIVER_LIBRARY} = full path to mysql-connector-java-*.jar
        jdbc_driver_library => "${JDBC_DRIVER_LIBRARY}"
        jdbc_driver_class => "com.mysql.jdbc.Driver"

        # ${SQL_HOST} = hostname of SQL server
        # ${SQL_PORT} = port of SQL server
        # ${SQL_DATABASE} = database name on SQL server
        # ${SQL_USER} = SQL server username
        # ${SQL_PASSWORD} = SQL server user password
        jdbc_connection_string => "jdbc:mysql://${SQL_HOST}:${SQL_PORT}/${SQL_DATABASE}"
        jdbc_user => "${SQL_USER}"
        jdbc_password => "${SQL_PASSWORD}"
        
        # Inject these parameters into the sql file
        parameters => {}

        # Run as daemon with the following cron schedule
        # schedule = "* * * * *"

        # ${QUERIES_DIR} = path of directory containing SQL statement to execute
        statement_filepath => "${QUERIES_DIR}/dashboard.sql"

        # Keep tabs on the timestamps
        # ${LASTRUN_DIR} = path to where jdbc_logstash plugin should persist its run metadata
        last_run_metadata_path => "${LASTRUN_DIR}/dashboard"
        use_column_value => true
        tracking_column => "updated_at"
    }
}
filter {
    fingerprint {
        method => "SHA1"
        concatenate_sources => true
        source => ["rawchem_id", "data_document_id", "product_id", "puc_id"]
        target => "[@metadata][fingerprint]"
    }
    date {
        match => ["updated_at", "UNIX"]
        remove_field => ["updated_at"]
    }
}
output {
    elasticsearch {
        # ${ELASTIC_HOST} = hostname of Elasticsearch instance
        # ${ELASTIC_PORT} = port of Elasticsearch instance
        # ${ELASTIC_INDEX} = port of Elasticsearch instance
        hosts => ["${ELASTIC_HOST}:${ELASTIC_PORT}"]
        index => "${ELASTIC_INDEX}"
        document_id => "%{[@metadata][fingerprint]}"
    }
}
